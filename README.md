# ğŸ›¡ï¸ AI-Powered Behavior & Motion Detection System

Real-time video surveillance system that detects suspicious behavior using AI â€” combining person detection, tracking, face recognition, pose estimation, and action classification into a unified behavior scoring engine.

![Python](https://img.shields.io/badge/Python-3.10+-blue)
![YOLOv8](https://img.shields.io/badge/YOLOv8-Detection-green)
![MediaPipe](https://img.shields.io/badge/MediaPipe-Pose-orange)
![FastAPI](https://img.shields.io/badge/FastAPI-Dashboard-red)

---

## ğŸ¯ What Does It Do?

The system processes live video feeds (webcam or CCTV) and:

1. **Detects** every person in the frame using YOLOv8
2. **Tracks** them across frames with persistent IDs (DeepSORT)
3. **Recognizes** known faces against a pre-registered database (DeepFace)
4. **Estimates** body pose and skeleton joints (MediaPipe)
5. **Classifies** actions â€” standing, walking, bending, running, hiding objects, loitering
6. **Scores** behavior suspicion (0â€“100) by combining 5 weighted signals
7. **Alerts** in real-time with sound, visual overlays, and evidence video clips

---

## ğŸ”‘ Key Features

| Feature | Description |
|---------|-------------|
| **Person Detection & Tracking** | YOLOv8 + DeepSORT with trajectory paths and speed analysis |
| **Face Recognition** | DeepFace-based identification from a `known_faces/` database |
| **Pose Estimation** | 33-joint skeleton via MediaPipe for gesture analysis |
| **Action Classification** | LSTM model with heuristic fallback (works without training) |
| **Behavior Scoring Engine** | Multi-signal consensus: action (35%) + trajectory (25%) + pose (20%) + zone (15%) + time (5%) |
| **Zone Monitoring** | Define restricted/inventory/exit areas â€” detects entry, exit, lingering |
| **False Positive Reduction** | Temporal smoothing + persistence check + multi-signal consensus |
| **Real-time Alerts** | Sound + visual alerts with configurable severity thresholds |
| **Evidence Recording** | Auto-saves 20s video clips (10s before + 10s after alert) with metadata |
| **Web Dashboard** | Live video feed, alert panel, person tracking, behavior scores at `http://localhost:8000` |
| **Frame Preprocessing** | CLAHE + auto-gamma correction for low-light environments |

---

## ğŸ“ Project Structure

```
Behaviour Detection/
â”œâ”€â”€ main.py                  # Main pipeline â€” runs everything
â”œâ”€â”€ config.py                # All settings in one place
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ modules/
â”‚   â”œâ”€â”€ preprocessor.py      # Frame lighting normalization
â”‚   â”œâ”€â”€ detector.py          # YOLOv8 person detection
â”‚   â”œâ”€â”€ tracker.py           # DeepSORT multi-person tracking
â”‚   â”œâ”€â”€ recognizer.py        # DeepFace face recognition
â”‚   â”œâ”€â”€ pose_estimator.py    # MediaPipe skeleton extraction
â”‚   â”œâ”€â”€ action_classifier.py # LSTM + heuristic action classification
â”‚   â”œâ”€â”€ behavior_engine.py   # Multi-signal behavior scoring
â”‚   â”œâ”€â”€ zone_manager.py      # Restricted zone monitoring
â”‚   â”œâ”€â”€ alert_system.py      # Alert triggering & notifications
â”‚   â””â”€â”€ evidence_recorder.py # Video clip capture on alerts
â”œâ”€â”€ dashboard/
â”‚   â”œâ”€â”€ app.py               # FastAPI backend + WebSocket
â”‚   â””â”€â”€ static/
â”‚       â”œâ”€â”€ index.html       # Dashboard UI
â”‚       â”œâ”€â”€ style.css         # Dark theme styling
â”‚       â””â”€â”€ app.js           # Frontend logic
â”œâ”€â”€ training/
â”‚   â””â”€â”€ train_action_model.py # LSTM training pipeline
â”œâ”€â”€ known_faces/             # Face images for recognition
â”‚   â””â”€â”€ <person_name>/
â”‚       â””â”€â”€ photo.jpg
â””â”€â”€ evidence/                # Auto-saved alert video clips
```

---

## ğŸš€ How to Run

### 1. Install Dependencies

```bash
# Create virtual environment (recommended)
python -m venv venv
venv\Scripts\activate        # Windows
# source venv/bin/activate   # Linux/Mac

# Install packages
pip install -r requirements.txt
```

### 2. Run the System

```bash
# Run with default webcam
python main.py

# Run with a video file
python main.py --source path/to/video.mp4

# Run without dashboard
python main.py --no-dashboard
```

### 3. Open Dashboard

Navigate to **http://localhost:8000** in your browser for the live monitoring dashboard.

### 4. Keyboard Controls

| Key | Action |
|-----|--------|
| `q` | Quit the system |
| `r` | Register a face from the live feed |
| `z` | Toggle zone overlay |
| `s` | Toggle skeleton drawing |

---

## ğŸ‘¤ Register Known Faces

Create a subfolder inside `known_faces/` with the person's name, and add face photos:

```
known_faces/
â”œâ”€â”€ Ahmad/
â”‚   â”œâ”€â”€ photo1.jpg
â”‚   â””â”€â”€ photo2.jpg
â”œâ”€â”€ Employee1/
â”‚   â””â”€â”€ face.jpg
```

> **Tip:** Add 2-3 photos per person from different angles for better accuracy.

---

## ğŸ§  How Behavior Scoring Works

The system evaluates each tracked person using **5 signals**, combined into a 0â€“100 score:

```
Behavior Score = Action (35%) + Trajectory (25%) + Pose (20%) + Zone (15%) + Time (5%)
```

| Score Range | Alert Level | What Happens |
|-------------|-------------|--------------|
| 0â€“39 | None | Normal activity |
| 40â€“64 | ğŸŸ¡ Low | Logged, mild visual indicator |
| 65â€“84 | ğŸŸ  Medium | Sound alert + dashboard notification |
| 85â€“100 | ğŸ”´ High | Full alert + evidence video recording |

### False Positive Reduction
- **Multi-signal consensus** â€” Single suspicious signal won't trigger high alerts
- **Temporal smoothing** â€” Score averaged over a sliding window
- **Persistence check** â€” Must stay suspicious for several consecutive frames
- **Cooldown** â€” Same person can't re-trigger alerts within 30 seconds

---

## ğŸ‹ï¸ Train Custom Action Model

The heuristic classifier works out of the box. For better accuracy, train the LSTM on your own data:

```bash
# Step 1: Collect labeled data from a video
python training/train_action_model.py --collect --source 0

# Step 2: Train the model
python training/train_action_model.py --train --data training/collected_data.npz

# Step 3: Evaluate
python training/train_action_model.py --evaluate --data training/collected_data.npz
```

---

## âš™ï¸ Configuration

All settings are centralized in **`config.py`**. Key parameters:

| Setting | Default | Description |
|---------|---------|-------------|
| `video_source` | `0` | Webcam index or video file path |
| `detection.confidence` | `0.5` | YOLOv8 detection threshold |
| `behavior.alert_threshold_high` | `85.0` | High alert trigger score |
| `alert.cooldown_seconds` | `30.0` | Cooldown between alerts for same person |
| `evidence.pre_buffer_seconds` | `10.0` | Seconds recorded before alert |

---

## ğŸ› ï¸ Tech Stack

- **YOLOv8** â€” Person Detection (Ultralytics)
- **DeepSORT** â€” Multi-Person Tracking
- **DeepFace** â€” Face Recognition (ArcFace model)
- **MediaPipe** â€” Pose Estimation (33 landmarks)
- **PyTorch** â€” LSTM Action Classifier
- **FastAPI** â€” Dashboard Backend + WebSocket
- **OpenCV** â€” Video Processing & Frame Rendering

---

## ğŸ“„ License

This project is for educational and research purposes.
